{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Training Process\n",
    "'''\n",
    "from classification_utils import *\n",
    "from text_rank_utils import *\n",
    "\n",
    "\n",
    "df =pd.read_pickle('data_pickle.pkl')\n",
    "df_total = data_processing(df)\n",
    "df_train, df_test = split_data(df_total)\n",
    "X_train, X_val, Y_train, Y_val = split_train_data(df_train)\n",
    "vocab = create_vocab(X_train,True)\n",
    "vocab =  load_vocab('vocab.txt')\n",
    "\n",
    "X_train = process_docs(X_train, vocab)\n",
    "X_val = process_docs(X_val, vocab)\n",
    "X_train,X_val,tokenizer_object = pipeline_tokenizer_matrix(X_train,X_val)\n",
    "X_train,X_val,pca_object = pipeline_pca(X_train,X_val,300)\n",
    "history,acc,model = pipeline_modelling_bow(X_train,X_val,Y_train,Y_val,0)\n",
    "print('validation_accuracy',acc)\n",
    "Y_pred_test_b,Y_test,acc_test = testing_data_bow(model,df_test,tokenizer_object,pca_object,vocab)\n",
    "print('testing_accuracy',acc_test)\n",
    "\n",
    "\n",
    "with open('model_objects.pkl', 'wb') as f:\n",
    "    pickle.dump([pca_object,tokenizer_object], f)\n",
    "\n",
    "\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights(\"model.h5\")\n",
    "\n",
    "#loadmodel\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "\n",
    "Y_pred_test_b,Y_test,acc_test = testing_data_bow(loaded_model,df_test,tokenizer_object,pca_object,vocab)\n",
    "print('loaded_model_testing_accuracy',acc_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/kowshikchilamkurthy/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/kowshikchilamkurthy/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/kowshikchilamkurthy/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/kowshikchilamkurthy/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/kowshikchilamkurthy/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/kowshikchilamkurthy/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/kowshikchilamkurthy/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/kowshikchilamkurthy/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/kowshikchilamkurthy/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/kowshikchilamkurthy/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/kowshikchilamkurthy/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/kowshikchilamkurthy/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from classification_utils import *\n",
    "from text_rank_utils import *\n",
    "from gensim.summarization import keywords\n",
    "\n",
    "\n",
    "def create_keys(text,word_count,summary_limit):\n",
    "    vocab =  load_vocab('vocab.txt')\n",
    "    json_file = open('model.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    loaded_model.load_weights(\"model.h5\")\n",
    "    \n",
    "    with open('model_objects.pkl', 'rb') as f:\n",
    "        model_objects_list = pickle.load(f)\n",
    "    pca_object       = model_objects_list[0]\n",
    "    tokenizer_object = model_objects_list[1]\n",
    "\n",
    "    article = clean(text,no_urls=True,replace_with_url=\"\",lower=False)\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(article)\n",
    "    summary = CentroidWordEmbeddingsSummarizer(BaseSummarizer).summarize(text=article,limit_type = 'word',limit=int(len(tokens)*summary_limit))\n",
    "    probability,label =  prediction_bow(loaded_model,text,tokenizer_object,pca_object,vocab)\n",
    "    keywords_text = keywords(text,words =word_count).split('\\n')\n",
    "    return(summary,probability[0],label,keywords_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Challenges in natural language processing frequently involve speech recognition, natural language understanding, natural language generation (frequently from formal, machine-readable logical forms),connecting language and machine perception, dialog systems, or somecombination thereof.'\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kowshikchilamkurthy/.local/lib/python3.5/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator PCA from version 0.20.2 when using version 0.21.3. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "summary,probability_label,label,keywords_list = create_keys(text,10,0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/kowshikchilamkurthy/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/kowshikchilamkurthy/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/kowshikchilamkurthy/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/kowshikchilamkurthy/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/kowshikchilamkurthy/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/kowshikchilamkurthy/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/kowshikchilamkurthy/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/kowshikchilamkurthy/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/kowshikchilamkurthy/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/kowshikchilamkurthy/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/kowshikchilamkurthy/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/kowshikchilamkurthy/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kowshikchilamkurthy/.local/lib/python3.5/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator PCA from version 0.20.2 when using version 0.21.3. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kowshikchilamkurthy/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "ename": "InvalidDocument",
     "evalue": "cannot encode object: 1.509361e-05, of type: <class 'numpy.float32'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidDocument\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-732808fa2da8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mnew_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'$set'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'summary'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'probability_label'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mprobability_label\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'keywords_list'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mkeywords_list\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mupdate_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'_id'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mObjectId\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0mmycol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdate_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnew_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pymongo/collection.py\u001b[0m in \u001b[0;36mupdate_one\u001b[0;34m(self, filter, update, upsert, bypass_document_validation, collation, array_filters, session)\u001b[0m\n\u001b[1;32m   1001\u001b[0m                 \u001b[0mbypass_doc_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbypass_document_validation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1002\u001b[0m                 \u001b[0mcollation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marray_filters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marray_filters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m                 session=session),\n\u001b[0m\u001b[1;32m   1004\u001b[0m             write_concern.acknowledged)\n\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pymongo/collection.py\u001b[0m in \u001b[0;36m_update_retryable\u001b[0;34m(self, criteria, document, upsert, check_keys, multi, manipulate, write_concern, op_id, ordered, bypass_doc_val, collation, array_filters, session)\u001b[0m\n\u001b[1;32m    854\u001b[0m         return self.__database.client._retryable_write(\n\u001b[1;32m    855\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mwrite_concern\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_concern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macknowledged\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmulti\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             _update, session)\n\u001b[0m\u001b[1;32m    857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m     def replace_one(self, filter, replacement, upsert=False,\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pymongo/mongo_client.py\u001b[0m in \u001b[0;36m_retryable_write\u001b[0;34m(self, retryable, func, session)\u001b[0m\n\u001b[1;32m   1490\u001b[0m         \u001b[0;34m\"\"\"Internal retryable write helper.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1491\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tmp_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1492\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retry_with_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretryable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1494\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_reset_server\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maddress\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pymongo/mongo_client.py\u001b[0m in \u001b[0;36m_retry_with_session\u001b[0;34m(self, retryable, func, session, bulk)\u001b[0m\n\u001b[1;32m   1383\u001b[0m                             \u001b[0;32mraise\u001b[0m \u001b[0mlast_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m                         \u001b[0mretryable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1385\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretryable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1386\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mServerSelectionTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mis_retrying\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pymongo/collection.py\u001b[0m in \u001b[0;36m_update\u001b[0;34m(session, sock_info, retryable_write)\u001b[0m\n\u001b[1;32m    850\u001b[0m                 \u001b[0mbypass_doc_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbypass_doc_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    851\u001b[0m                 \u001b[0marray_filters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marray_filters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 852\u001b[0;31m                 retryable_write=retryable_write)\n\u001b[0m\u001b[1;32m    853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m         return self.__database.client._retryable_write(\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pymongo/collection.py\u001b[0m in \u001b[0;36m_update\u001b[0;34m(self, sock_info, criteria, document, upsert, check_keys, multi, manipulate, write_concern, op_id, ordered, bypass_doc_val, collation, array_filters, session, retryable_write)\u001b[0m\n\u001b[1;32m    820\u001b[0m             \u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m             \u001b[0mclient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__database\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 822\u001b[0;31m             retryable_write=retryable_write).copy()\n\u001b[0m\u001b[1;32m    823\u001b[0m         \u001b[0m_check_write_command_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m         \u001b[0;31m# Add the updatedExisting field for compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pymongo/pool.py\u001b[0m in \u001b[0;36mcommand\u001b[0;34m(self, dbname, spec, slave_ok, read_preference, codec_options, check, allowable_errors, check_keys, read_concern, write_concern, parse_write_concern_error, collation, session, client, retryable_write, publish_events, user_fields)\u001b[0m\n\u001b[1;32m    616\u001b[0m         \u001b[0;31m# Catch socket.error, KeyboardInterrupt, etc. and close ourselves.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 618\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_connection_failure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msend_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_doc_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pymongo/pool.py\u001b[0m in \u001b[0;36mcommand\u001b[0;34m(self, dbname, spec, slave_ok, read_preference, codec_options, check, allowable_errors, check_keys, read_concern, write_concern, parse_write_concern_error, collation, session, client, retryable_write, publish_events, user_fields)\u001b[0m\n\u001b[1;32m    611\u001b[0m                            \u001b[0muse_op_msg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop_msg_enabled\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m                            \u001b[0munacknowledged\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munacknowledged\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 613\u001b[0;31m                            user_fields=user_fields)\n\u001b[0m\u001b[1;32m    614\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOperationFailure\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pymongo/network.py\u001b[0m in \u001b[0;36mcommand\u001b[0;34m(sock, dbname, spec, slave_ok, is_mongos, read_preference, codec_options, session, client, check, allowable_errors, address, check_keys, listeners, max_bson_size, read_concern, parse_write_concern_error, collation, compression_ctx, use_op_msg, unacknowledged, user_fields)\u001b[0m\n\u001b[1;32m    127\u001b[0m         request_id, msg, size, max_doc_size = message._op_msg(\n\u001b[1;32m    128\u001b[0m             \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdbname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mread_preference\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslave_ok\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m             codec_options, ctx=compression_ctx)\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0;31m# If this is an unacknowledged write then make sure the encoded doc(s)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;31m# are small enough, otherwise rely on the server to return an error.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pymongo/message.py\u001b[0m in \u001b[0;36m_op_msg\u001b[0;34m(flags, command, dbname, read_preference, slave_ok, check_keys, opts, ctx)\u001b[0m\n\u001b[1;32m    702\u001b[0m                 flags, command, identifier, docs, check_keys, opts, ctx)\n\u001b[1;32m    703\u001b[0m         return _op_msg_uncompressed(\n\u001b[0;32m--> 704\u001b[0;31m             flags, command, identifier, docs, check_keys, opts)\n\u001b[0m\u001b[1;32m    705\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m         \u001b[0;31m# Add the field back to the command.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidDocument\u001b[0m: cannot encode object: 1.509361e-05, of type: <class 'numpy.float32'>"
     ]
    }
   ],
   "source": [
    "import pymongo\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import logging\n",
    "import pymongo\n",
    "import multiprocessing\n",
    "import dateparser\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm import tnrange,tqdm_notebook\n",
    "import tqdm\n",
    "from bson import ObjectId\n",
    "from classification_utils import *\n",
    "from text_rank_utils import *\n",
    "from gensim.summarization import keywords\n",
    "\n",
    "\n",
    "def create_keys(text,word_count,summary_limit):\n",
    "    vocab =  load_vocab('vocab.txt')\n",
    "    json_file = open('model.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    loaded_model.load_weights(\"model.h5\")\n",
    "    \n",
    "    with open('model_objects.pkl', 'rb') as f:\n",
    "        model_objects_list = pickle.load(f)\n",
    "    pca_object       = model_objects_list[0]\n",
    "    tokenizer_object = model_objects_list[1]\n",
    "\n",
    "    article = clean(text,no_urls=True,replace_with_url=\"\",lower=False)\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(article)\n",
    "    summary = CentroidWordEmbeddingsSummarizer(BaseSummarizer).summarize(text=article,limit_type = 'word',limit=int(len(tokens)*summary_limit))\n",
    "    probability,label =  prediction_bow(loaded_model,text,tokenizer_object,pca_object,vocab)\n",
    "    keywords_text = keywords(text,words =word_count).split('\\n')\n",
    "    return(summary,probability[0],label,keywords_text)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "words_count = 10\n",
    "summary_shrink = 0.25\n",
    "def create_keys_article_mongo(text,words_count,summary_shrink):\n",
    "    try:\n",
    "        summary,probability_label,label,keywords_list = create_keys(text,words_count,summary_shrink)\n",
    "\n",
    "    except:\n",
    "        summary='None'\n",
    "        probability_label = 'None' \n",
    "        label= 'None' \n",
    "        keywords_list='None'  \n",
    "    return(summary,probability_label,label,keywords_list)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "myclient = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "mydb = myclient[\"aigk\"]\n",
    "mycol = mydb[\"articles\"]\n",
    "articles_collection = mycol.find({\"summary\":{'$exists': False}})\n",
    "\n",
    "a = 0\n",
    "for i in articles_collection:\n",
    "    if i['source_id'] != 'forum_ias':\n",
    "        a = a+1\n",
    "        print(a)\n",
    "        if i['source_id'] == 'businessline':\n",
    "            total_string = ''\n",
    "            for j in i['text']:\n",
    "                total_string = total_string +' '+ j\n",
    "            text_article = total_string\n",
    "            \n",
    "        elif i['source_id'] == 'gktoday':\n",
    "            a = i['text']\n",
    "            if ';' in a:\n",
    "                text_article = a.split(';')[1]\n",
    "            else:\n",
    "                text_article = a\n",
    "        else:\n",
    "            text_article=i['text']\n",
    "                \n",
    "        \n",
    "        try:\n",
    "            summary,probability_label,label,keywords_list = create_keys_article_mongo(text_article,words_count,summary_shrink)\n",
    "        \n",
    "        except:\n",
    "            summary='None_2'\n",
    "            probability_label = 'None_2' \n",
    "            label= 'None_2' \n",
    "            keywords_list='None_2'  \n",
    "        \n",
    "        new_keys={'$set':{'summary':summary,'probability_label':float(probability_label),'label':float(label),'keywords_list':keywords_list}}\n",
    "        update_id={'_id': ObjectId(i['_id'])}\n",
    "        mycol.update_one(update_id,new_keys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(new_keys['$set']['summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "float"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(float(new_keys['$set']['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import logging\n",
    "import pymongo\n",
    "import multiprocessing\n",
    "import dateparser\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm import tnrange,tqdm_notebook\n",
    "import tqdm\n",
    "from bson import ObjectId\n",
    "from classification_utils import *\n",
    "from text_rank_utils import *\n",
    "from gensim.summarization import keywords\n",
    "\n",
    "\n",
    "def create_keys(text,word_count,summary_limit,pca_object,tokenizer_object,vocab,loaded_model):\n",
    "    article = clean(text,no_urls=True,replace_with_url=\"\",lower=False)\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(article)\n",
    "    summary = CentroidWordEmbeddingsSummarizer(BaseSummarizer).summarize(text=article,limit_type = 'word',limit=int(len(tokens)*summary_limit))\n",
    "    probability,label =  prediction_bow(loaded_model,text,tokenizer_object,pca_object,vocab)\n",
    "    keywords_text = keywords(text,words =word_count).split('\\n')\n",
    "    return(summary,float(probability[0]),float(label),keywords_text)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "words_count = 10\n",
    "summary_shrink = 0.25\n",
    "\n",
    "\n",
    "with open('model_objects.pkl', 'rb') as f:\n",
    "    model_objects_list = pickle.load(f)\n",
    "pca_object       = model_objects_list[0]\n",
    "tokenizer_object = model_objects_list[1]\n",
    "    \n",
    "vocab =  load_vocab('vocab.txt')\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "    \n",
    "    \n",
    "def create_keys_article_mongo(text,words_count,summary_shrink,pca_object,tokenizer_object,vocab,loaded_model):\n",
    "    try:\n",
    "        summary,probability_label,label,keywords_list = create_keys(text,words_count,summary_shrink,pca_object,tokenizer_object,vocab,loaded_model)\n",
    "        \n",
    "    except:\n",
    "        summary='None'\n",
    "        probability_label = 'None' \n",
    "        label= 'None' \n",
    "        keywords_list='None'  \n",
    "    return(summary,probability_label,label,keywords_list)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "myclient = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "mydb = myclient[\"aigk\"]\n",
    "mycol = mydb[\"articles\"]\n",
    "articles_collection = mycol.find({\"summary\":{'$exists': False}})\n",
    "\n",
    "a = 0\n",
    "for i in articles_collection:\n",
    "    if i['source_id'] != 'forum_ias':\n",
    "        a = a+1\n",
    "        print(a)\n",
    "        if i['source_id'] == 'businessline':\n",
    "            total_string = ''\n",
    "            for j in i['text']:\n",
    "                total_string = total_string +' '+ j\n",
    "            text_article = total_string\n",
    "            \n",
    "        elif i['source_id'] == 'gktoday':\n",
    "            a = i['text']\n",
    "            if ';' in a:\n",
    "                text_article = a.split(';')[1]\n",
    "            else:\n",
    "                text_article = a\n",
    "        else:\n",
    "            text_article=i['text']\n",
    "                \n",
    "        \n",
    "        try:\n",
    "            summary,probability_label,label,keywords_list = create_keys_article_mongo(text_article,words_count,summary_shrink,pca_object,tokenizer_object,vocab,loaded_model)\n",
    "        \n",
    "        except:\n",
    "            summary='None_2'\n",
    "            probability_label = 'None_2' \n",
    "            label= 'None_2' \n",
    "            keywords_list='None_2'  \n",
    "        \n",
    "        if (summary == 'None') or (summary == 'None_2'):\n",
    "            print(\"none noted\")\n",
    "        new_keys={'$set':{'summary':summary,'probability_label':probability_label,'label':label,'keywords_list':keywords_list}}\n",
    "        update_id={'_id': ObjectId(i['_id'])}\n",
    "        mycol.update_one(update_id,new_keys)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
